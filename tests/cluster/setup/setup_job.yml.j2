custom:
  spark-conf-single: &spark-conf-single
    spark.databricks.io.cache.enabled: true
    spark.databricks.delta.preview.enabled: true
    spark.databricks.cluster.profile: singleNode
    spark.master: local[*, 4]

  azure-attributes: &azure-attributes
    availability: ON_DEMAND_AZURE
    first_on_demand: 1
    spot_bid_max_price: -1

  basic-cluster-single: &basic-cluster-single
    spark_version: "9.1.x-scala2.12"
    spark_conf: *spark-conf-single
    azure_attributes: *azure-attributes
    spark_env_vars:
      PYSPARK_PYTHON: /databricks/python3/bin/python3
    node_type_id: "Standard_DS3_v2"
    num_workers: 0



environments:
  default:
    strict_path_adjustment_policy: true
    jobs:
      - name: Setup Mounts
        new_cluster:
          <<: *basic-cluster-single
          custom_tags:
            JobName: Setup Mounts

        max_retries: 0
        spark_python_task:
          python_file: "file://src/spetlr/mount/main.py"
          parameters: [ "file:fuse://tests/cluster/setup/mounts.json"]

      - name: Setup Gp Cluster
        new_cluster:
          <<: *basic-cluster-single
          custom_tags:
            JobName: Setup Gp Cluster

        max_retries: 0
        python_wheel_task:
          package_name: spetlr
          entry_point: spetlr_deploy_gp_cluster_from_job
          named_parameters:
            autotermination_minutes: "10"
            cluster_name: jobdeploy
